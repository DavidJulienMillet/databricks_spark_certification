{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:2em;\"> Databricks Certified Associate Developer for Apache Spark 3.0 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no official documentation indicating what we are supposed to know to pass the databricks Certified Associate Developer for Apache Spark 3.0. However databricks website advises to take 4 courses which cost each $1500. As I don't want to take them, I record the table of contents and try to learn all the points reading the documentation. https://spark.apache.org/docs/latest/ \\\n",
    "The purpose of this notebook is to test all the codes of the official documentation, try the functions and options  when no example are provided and see when and why it works or doesn't.\n",
    "\n",
    "__Table of contents of the databrick courses__\n",
    "\n",
    "Courses from https://academy.databricks.com/instructor-led-training/apache-spark-programming\n",
    "* Day 1: DataFrames\n",
    " * __1 Introduction__ Course overview, Databricks ecosystem, Spark overview, Case study, Knowledge check\n",
    " * __2 Databricks Platform__ Databricks concepts, Workspace UI, Notebooks, Lab\n",
    " * __3 Spark SQL__ Spark SQL module, Documentation, DataFrame concepts, Lab\n",
    " * __4 Reader & Writer__ Data Sources, DataFrameReader & Writer, Schemas, Performance, Lab\n",
    " * __5 DataFrame & Column__ Columns and expressions, Transformations, Actions, Rows, Lab\n",
    " \n",
    "* Day 2: Transformations\n",
    " * __1 Aggregation__ Groupby, Grouped data methods, Aggregate functions, Math functions, Lab\n",
    " * __2 Datetimes__ Dates & Timestamps, Datetime patterns, Datetime functions, Lab\n",
    " * __3 Complex Types__ String functions, Collection functions\n",
    " * __4 Additional Function__s Non-aggregate functions, NaFunctions, Lab\n",
    " * __5 User-Defined Functions__ User-defined functions, Vectorized UDFs, Performance, Lab\n",
    " \n",
    "* Day 3: Spark Optimization\n",
    " * __1 Spark Architecture__ Spark Cluster, Spark Execution, Shuffling, Lab\n",
    " * __2 Shuffles & Caching__ Lineage, Shuffle files, Caching, Caching recommendations, Spark UI: Storage, Lab\n",
    " * __3 Query Optimization__ Catalyst Optimizer, Adaptive Query Execution, Best practices, Lab\n",
    " * __4 Spark UI__ Spark UI navigation, Spark UI: Jobs, Stages, SQL\n",
    " * __5 Partitioning__ Partitions vs cores, Default shuffle partitions, Repartition, Best practices, AQE, Lab\n",
    " \n",
    "* Day 4: Structured Streaming\n",
    " * __1 Review__ DataFrames and Transformations, Lab\n",
    " * __2 Streaming Query__ Streaming concepts, Sources and Sinks, Streaming Query, Transformations, Lab\n",
    " * __3 Processing Streams__ Monitoring Streams, Lab\n",
    " * __4 Aggregating Streams__ Streaming aggregations, Windows, Watermarking, Lab\n",
    " * __5 Delta Lake__ Delta Lake concepts, Batch and streaming, Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Spark-Architecture\" data-toc-modified-id=\"Spark-Architecture-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Spark Architecture</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataframes-and-RDD\" data-toc-modified-id=\"Dataframes-and-RDD-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Dataframes and RDD</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-Dataframes\" data-toc-modified-id=\"Create-Dataframes-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Create Dataframes</a></span></li><li><span><a href=\"#Dataframe-to-pandas-and-pandas-to-df\" data-toc-modified-id=\"Dataframe-to-pandas-and-pandas-to-df-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Dataframe to pandas and pandas to df</a></span></li><li><span><a href=\"#RDD-Distributed-dataframe\" data-toc-modified-id=\"RDD-Distributed-dataframe-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>RDD Distributed dataframe</a></span></li></ul></li><li><span><a href=\"#Parallelization-principle\" data-toc-modified-id=\"Parallelization-principle-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Parallelization principle</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transformations,-Actions,-Shuffling\" data-toc-modified-id=\"Transformations,-Actions,-Shuffling-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Transformations, Actions, Shuffling</a></span></li><li><span><a href=\"#RDD-lazyness\" data-toc-modified-id=\"RDD-lazyness-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>RDD lazyness</a></span></li><li><span><a href=\"#Caching\" data-toc-modified-id=\"Caching-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Caching</a></span></li><li><span><a href=\"#Broadcast\" data-toc-modified-id=\"Broadcast-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Broadcast</a></span></li><li><span><a href=\"#Accumulator\" data-toc-modified-id=\"Accumulator-1.2.5\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>Accumulator</a></span></li></ul></li><li><span><a href=\"#Partitioning-Partitions-vs-cores\" data-toc-modified-id=\"Partitioning-Partitions-vs-cores-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Partitioning Partitions vs cores</a></span><ul class=\"toc-item\"><li><span><a href=\"#Nodes-architecture\" data-toc-modified-id=\"Nodes-architecture-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Nodes architecture</a></span></li><li><span><a href=\"#Task,-partition,-executor-optimization\" data-toc-modified-id=\"Task,-partition,-executor-optimization-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Task, partition, executor optimization</a></span></li></ul></li></ul></li><li><span><a href=\"#DataFrames\" data-toc-modified-id=\"DataFrames-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>DataFrames</a></span><ul class=\"toc-item\"><li><span><a href=\"#Reader-&amp;-Writer\" data-toc-modified-id=\"Reader-&amp;-Writer-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Reader &amp; Writer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Read-one-text-file-or-the-whole-directory\" data-toc-modified-id=\"Read-one-text-file-or-the-whole-directory-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Read one text file or the whole directory</a></span></li><li><span><a href=\"#Read-and-save-a-RDD-as-squence-file\" data-toc-modified-id=\"Read-and-save-a-RDD-as-squence-file-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Read and save a RDD as squence file</a></span></li><li><span><a href=\"#Read-and-save-a-dataframe---csv\" data-toc-modified-id=\"Read-and-save-a-dataframe---csv-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Read and save a dataframe - csv</a></span></li><li><span><a href=\"#Read-and-save-a-json\" data-toc-modified-id=\"Read-and-save-a-json-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Read and save a json</a></span></li><li><span><a href=\"#Read-and-save-a-parquet\" data-toc-modified-id=\"Read-and-save-a-parquet-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Read and save a parquet</a></span></li></ul></li><li><span><a href=\"#DataFrame-&amp;-Column-Columns-and-expressions,-Transformations,-Actions,-Rows,\" data-toc-modified-id=\"DataFrame-&amp;-Column-Columns-and-expressions,-Transformations,-Actions,-Rows,-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>DataFrame &amp; Column Columns and expressions, Transformations, Actions, Rows,</a></span><ul class=\"toc-item\"><li><span><a href=\"#Select-columns\" data-toc-modified-id=\"Select-columns-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Select columns</a></span></li><li><span><a href=\"#DataFrame-filter-rows\" data-toc-modified-id=\"DataFrame-filter-rows-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>DataFrame filter rows</a></span></li><li><span><a href=\"#Drop,-rename,-replace-columns\" data-toc-modified-id=\"Drop,-rename,-replace-columns-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Drop, rename, replace columns</a></span></li><li><span><a href=\"#Replace-value,-replace-na\" data-toc-modified-id=\"Replace-value,-replace-na-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Replace value, replace na</a></span></li><li><span><a href=\"#Create-dataframe\" data-toc-modified-id=\"Create-dataframe-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Create dataframe</a></span></li><li><span><a href=\"#Schemas\" data-toc-modified-id=\"Schemas-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Schemas</a></span></li><li><span><a href=\"#Dataframe-rows\" data-toc-modified-id=\"Dataframe-rows-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>Dataframe rows</a></span></li></ul></li><li><span><a href=\"#Aggregation-Groupby,-Grouped-data-methods,-Aggregate-functions\" data-toc-modified-id=\"Aggregation-Groupby,-Grouped-data-methods,-Aggregate-functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Aggregation Groupby, Grouped data methods, Aggregate functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Merge,-joins\" data-toc-modified-id=\"Merge,-joins-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Merge, joins</a></span></li><li><span><a href=\"#Aggregation\" data-toc-modified-id=\"Aggregation-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Aggregation</a></span></li></ul></li><li><span><a href=\"#Datetimes-Dates,-string\" data-toc-modified-id=\"Datetimes-Dates,-string-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Datetimes Dates, string</a></span><ul class=\"toc-item\"><li><span><a href=\"#Datetime\" data-toc-modified-id=\"Datetime-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Datetime</a></span></li><li><span><a href=\"#Complex-Types-String-functions\" data-toc-modified-id=\"Complex-Types-String-functions-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Complex Types String functions</a></span></li><li><span><a href=\"#Lit\" data-toc-modified-id=\"Lit-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Lit</a></span></li></ul></li><li><span><a href=\"#User-Defined-Functions-User-defined-functions,-Vectorized-UDFs,-Performance\" data-toc-modified-id=\"User-Defined-Functions-User-defined-functions,-Vectorized-UDFs,-Performance-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>User-Defined Functions User-defined functions, Vectorized UDFs, Performance</a></span><ul class=\"toc-item\"><li><span><a href=\"#User-defined-functions\" data-toc-modified-id=\"User-defined-functions-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>User-defined functions</a></span></li><li><span><a href=\"#Vectorized-UDFs\" data-toc-modified-id=\"Vectorized-UDFs-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Vectorized UDFs</a></span></li><li><span><a href=\"#Performance\" data-toc-modified-id=\"Performance-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Performance</a></span></li></ul></li><li><span><a href=\"#Spark-SQL\" data-toc-modified-id=\"Spark-SQL-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Spark SQL</a></span></li></ul></li><li><span><a href=\"#Streaming\" data-toc-modified-id=\"Streaming-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Streaming</a></span><ul class=\"toc-item\"><li><span><a href=\"#Principle\" data-toc-modified-id=\"Principle-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Principle</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example\" data-toc-modified-id=\"Example-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Example</a></span></li><li><span><a href=\"#Discretized-stream\" data-toc-modified-id=\"Discretized-stream-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Discretized stream</a></span></li></ul></li><li><span><a href=\"#Input-/-output\" data-toc-modified-id=\"Input-/-output-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Input / output</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-DStreams-and-Receivers\" data-toc-modified-id=\"Input-DStreams-and-Receivers-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Input DStreams and Receivers</a></span></li><li><span><a href=\"#Basic-sources\" data-toc-modified-id=\"Basic-sources-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Basic sources</a></span></li><li><span><a href=\"#Queue-of-RDD-for-test-purpose\" data-toc-modified-id=\"Queue-of-RDD-for-test-purpose-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>Queue of RDD for test purpose</a></span></li><li><span><a href=\"#Receiver-Reliability\" data-toc-modified-id=\"Receiver-Reliability-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>Receiver Reliability</a></span></li></ul></li><li><span><a href=\"#Transformations-on-DStreams\" data-toc-modified-id=\"Transformations-on-DStreams-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Transformations on DStreams</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classical-transformations-example\" data-toc-modified-id=\"Classical-transformations-example-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Classical transformations example</a></span></li><li><span><a href=\"#updateStateByKey-transformation\" data-toc-modified-id=\"updateStateByKey-transformation-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>updateStateByKey transformation</a></span></li><li><span><a href=\"#transform(func)-operations\" data-toc-modified-id=\"transform(func)-operations-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;</span>transform(func) operations</a></span></li><li><span><a href=\"#Window-Operations\" data-toc-modified-id=\"Window-Operations-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;</span>Window Operations</a></span></li></ul></li><li><span><a href=\"#Join-Stream-operation\" data-toc-modified-id=\"Join-Stream-operation-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Join Stream operation</a></span><ul class=\"toc-item\"><li><span><a href=\"#join-all-the-rdds-of-the-stream\" data-toc-modified-id=\"join-all-the-rdds-of-the-stream-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>join all the rdds of the stream</a></span></li><li><span><a href=\"#join-the-rdds-of-specific-windows\" data-toc-modified-id=\"join-the-rdds-of-specific-windows-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>join the rdds of specific windows</a></span></li><li><span><a href=\"#transform:-stream-and-rdd-join\" data-toc-modified-id=\"transform:-stream-and-rdd-join-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;</span>transform: stream and rdd join</a></span></li></ul></li><li><span><a href=\"#Output-Operations-on-DStreams\" data-toc-modified-id=\"Output-Operations-on-DStreams-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Output Operations on DStreams</a></span></li><li><span><a href=\"#Design-Patterns-for-using-foreachRDD\" data-toc-modified-id=\"Design-Patterns-for-using-foreachRDD-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Design Patterns for using foreachRDD</a></span></li><li><span><a href=\"#DataFrame-and-SQL-Operations\" data-toc-modified-id=\"DataFrame-and-SQL-Operations-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>DataFrame and SQL Operations</a></span></li><li><span><a href=\"#Caching-/-Persistence\" data-toc-modified-id=\"Caching-/-Persistence-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Caching / Persistence</a></span></li><li><span><a href=\"#Checkpointing\" data-toc-modified-id=\"Checkpointing-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Checkpointing</a></span></li></ul></li><li><span><a href=\"#ML\" data-toc-modified-id=\"ML-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ML</a></span><ul class=\"toc-item\"><li><span><a href=\"#ML-fit,-transform,-pipelines,-estimator\" data-toc-modified-id=\"ML-fit,-transform,-pipelines,-estimator-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>ML fit, transform, pipelines, estimator</a></span><ul class=\"toc-item\"><li><span><a href=\"#Use-fit,-transform-on-StringIndexer-estimator\" data-toc-modified-id=\"Use-fit,-transform-on-StringIndexer-estimator-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Use fit, transform on StringIndexer estimator</a></span></li><li><span><a href=\"#Use-a-second-fit-transform-on-OneHotEncoder-estimator\" data-toc-modified-id=\"Use-a-second-fit-transform-on-OneHotEncoder-estimator-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Use a second fit transform on OneHotEncoder estimator</a></span></li><li><span><a href=\"#Use-a-pipeline-to-make-different-transformation-in-a-row\" data-toc-modified-id=\"Use-a-pipeline-to-make-different-transformation-in-a-row-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Use a pipeline to make different transformation in a row</a></span></li></ul></li><li><span><a href=\"#Model-Selection,-evaluator,-parameter-grid\" data-toc-modified-id=\"Model-Selection,-evaluator,-parameter-grid-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Model Selection, evaluator, parameter grid</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-train-and-validation-datasets\" data-toc-modified-id=\"Split-train-and-validation-datasets-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Split train and validation datasets</a></span></li><li><span><a href=\"#Train-and-estimate-model-with-fixed-parameters\" data-toc-modified-id=\"Train-and-estimate-model-with-fixed-parameters-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Train and estimate model with fixed parameters</a></span></li><li><span><a href=\"#Cross-validation-selection-to-find-best-parameters\" data-toc-modified-id=\"Cross-validation-selection-to-find-best-parameters-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Cross validation selection to find best parameters</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes taken from this documentation: \\\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes and RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a local StreamingContext if it has not been launched previously\n",
    "try:\n",
    "    conf = SparkConf().setAppName(\"NetworkWordCount\").setMaster(\"local[2]\")\n",
    "    sc = SparkContext(conf=SparkConf())\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "\n",
    "# Need to use SparkSession(sc) to create DataFrame\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|column1|column2|\n",
      "+-------+-------+\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates an empty dataframe\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"column1\", StringType(), True),\n",
    "    StructField(\"column2\", StringType(), True)\n",
    "])\n",
    "df = spark.createDataFrame(sc.emptyRDD(), schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      WordList|\n",
      "+--------------+\n",
      "|[Hello, world]|\n",
      "| [I, am, fine]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "# Creates a dataframe from list\n",
    "cSchema = StructType([StructField(\"WordList\", ArrayType(StringType()))])\n",
    "\n",
    "# notice extra square brackets around each element of list \n",
    "test_list = [['Hello', 'world']], [['I', 'am', 'fine']]\n",
    "\n",
    "df = spark.createDataFrame(test_list, schema=cSchema) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to pandas and pandas to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Hello, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, am, fine]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         WordList\n",
       "0  [Hello, world]\n",
       "1   [I, am, fine]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converts the dataframe into pandas dataframe\n",
    "df_pd = df.toPandas()\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList    [Hello, world]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Still work even if pandas has never been imported\n",
    "df_pd.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  0  1  2\n",
       "1  3  4  5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converts the pandas dataframe into spark dataframe \n",
    "import pandas as pd\n",
    "df_pd = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[\"a\", \"b\", \"c\"])\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: bigint, c: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(df_pd)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  0|  1|  2|\n",
      "|  3|  4|  5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Distributed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[23] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiation\n",
    "data = [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
    "distData = sc.parallelize(data)\n",
    "distData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action on the RDD\n",
    "distData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[32] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD from dataframe which is from pandas\n",
    "df_pd = pd.DataFrame([[0, 1, 2], [3, 4, 5]], columns=[\"a\", \"b\", \"c\"])\n",
    "df = spark.createDataFrame(df_pd)\n",
    "rdd = df.rdd\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=0, b=1, c=2), Row(a=3, b=4, c=5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  0|  1|  2|\n",
      "|  3|  4|  5|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD to DF\n",
    "df = rdd.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  0  1  2\n",
       "1  3  4  5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD to pandas through  df\n",
    "df_pd = df.toPandas()\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations, Actions, Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main principle of the parallelisation is to split a RDD into many partitions. The RDD works with a key/value pair system. Transformation are applied, then a shuffling and a reduction to get the result through an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*dlsSJ3LQ1l70S_VfGzeM0Q.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_from https://medium.com/@millet.david.julien/how-to-use-pyspark-to-get-opencv-images-descriptors-with-aws-emr-dd3875503a75_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input\n",
    "text = \"we have a string with words and we want to get the number of occurence \\\n",
    "of the words\"\n",
    "lst_text = text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 1),\n",
       " ('have', 1),\n",
       " ('a', 1),\n",
       " ('string', 1),\n",
       " ('with', 1),\n",
       " ('words', 1),\n",
       " ('and', 1),\n",
       " ('we', 1),\n",
       " ('want', 1),\n",
       " ('to', 1),\n",
       " ('get', 1),\n",
       " ('the', 1),\n",
       " ('number', 1),\n",
       " ('of', 1),\n",
       " ('occurence', 1),\n",
       " ('of', 1),\n",
       " ('the', 1),\n",
       " ('words', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping\n",
    "rdd_text = sc.parallelize(lst_text)\n",
    "rdd_text_map = rdd_text.map(lambda w: (w, 1))\n",
    "rdd_text_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('have', 1),\n",
       " ('of', 2),\n",
       " ('a', 1),\n",
       " ('string', 1),\n",
       " ('words', 2),\n",
       " ('and', 1),\n",
       " ('to', 1),\n",
       " ('the', 2),\n",
       " ('number', 1),\n",
       " ('occurence', 1),\n",
       " ('we', 2),\n",
       " ('with', 1),\n",
       " ('want', 1),\n",
       " ('get', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffling and reducing\n",
    "rdd_text_red = rdd_text_map.reduceByKey(lambda a, b: a + b)\n",
    "rdd_text_red.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|    words|unique count|\n",
      "+---------+------------+\n",
      "|     have|           1|\n",
      "|       of|           2|\n",
      "|        a|           1|\n",
      "|   string|           1|\n",
      "|    words|           2|\n",
      "|      and|           1|\n",
      "|       to|           1|\n",
      "|      the|           2|\n",
      "|   number|           1|\n",
      "|occurence|           1|\n",
      "|       we|           2|\n",
      "|     with|           1|\n",
      "|     want|           1|\n",
      "|      get|           1|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Result in a dataframe\n",
    "df = rdd_text_red.toDF([\"words\", \"unique count\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>unique count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>have</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>string</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>words</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>number</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>occurence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>we</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>with</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>get</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words  unique count\n",
       "0        have             1\n",
       "1          of             2\n",
       "2           a             1\n",
       "3      string             1\n",
       "4       words             2\n",
       "5         and             1\n",
       "6          to             1\n",
       "7         the             2\n",
       "8      number             1\n",
       "9   occurence             1\n",
       "10         we             2\n",
       "11       with             1\n",
       "12       want             1\n",
       "13        get             1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result in a pandas df\n",
    "df_pd = df.toPandas()\n",
    "df_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD lazyness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD is computed only when an action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization duration = 3.4442 seconds\n",
      "Transformations duration = 0.0252 seconds\n",
      "Action duration = 0.5362 seconds\n",
      "Second action duration = 0.2229 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from loremipsum import get_sentences\n",
    "\n",
    "# Create an input\n",
    "begin = time.time()\n",
    "str_text = \" \".join(get_sentences(50000))\n",
    "lst_text = str_text.split(\" \")\n",
    "rdd_text = sc.parallelize(lst_text)\n",
    "print(\"Initialization duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Transformation, mapping and reducing\n",
    "begin = time.time()\n",
    "rdd_text_map = rdd_text.map(lambda w: (w, 1))\n",
    "rdd_text_red = rdd_text_map.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Transformations duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Action to collect\n",
    "begin = time.time()\n",
    "rdd_text_red.collect()\n",
    "print(\"Action duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Second action called to show the df after a rdd/df transformation\n",
    "df = rdd_text_red.toDF([\"words\", \"unique count\"])\n",
    "begin = time.time()\n",
    "df.collect()\n",
    "print(\"Second action duration = {:.4f} seconds\".format(time.time() - begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching after transformation and before the action save the transformations when they are executed after the first action call. The second action call is much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*toxoeQ4Rec4aAAcYKw55ow.jpeg\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_from https://medium.com/@millet.david.julien/how-to-use-pyspark-to-get-opencv-images-descriptors-with-aws-emr-dd3875503a75_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*WQaW52PWPiUKJ8tcIfb8vw.jpeg\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_from https://medium.com/@millet.david.julien/how-to-use-pyspark-to-get-opencv-images-descriptors-with-aws-emr-dd3875503a75_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization duration = 3.4269 seconds\n",
      "Transformations duration = 0.0374 seconds\n",
      "First action duration = 0.5840 seconds\n",
      "Second same action duration = 0.0494 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create an input\n",
    "begin = time.time()\n",
    "str_text = \" \".join(get_sentences(50000))\n",
    "lst_text = str_text.split(\" \")\n",
    "rdd_text = sc.parallelize(lst_text)\n",
    "print(\"Initialization duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Transformation, mapping and reducing\n",
    "begin = time.time()\n",
    "rdd_text_map = rdd_text.map(lambda w: (w, 1))\n",
    "rdd_text_red = rdd_text_map.reduceByKey(lambda a, b: a + b).cache()\n",
    "print(\"Transformations duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Action to collect\n",
    "begin = time.time()\n",
    "rdd_text_red.collect()\n",
    "print(\"First action duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Action to collect\n",
    "begin = time.time()\n",
    "rdd_text_red.collect()\n",
    "print(\"Second same action duration = {:.4f} seconds\".format(time.time() - begin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".cache() is the equivalent of .persist() called with the default option StorageLevel.MEMORY_ONLY \\\n",
    "The other options are\n",
    "\n",
    "* __MEMORY_ONLY__ Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed. This is the default level.\n",
    "* __MEMORY_AND_DISK__ Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.\n",
    "* __MEMORY_ONLY_SER__ (Java and Scala) Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.\n",
    "* __MEMORY_AND_DISK_SER__ (Java and Scala) Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.\n",
    "* __DISK_ONLY__ Store the RDD partitions only on disk.\n",
    "* __MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.__ \tSame as the levels above, but replicate each partition on two cluster nodes.\n",
    "* __OFF_HEAP (experimental)__ Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization duration = 3.5446 seconds\n",
      "Transformations duration = 0.0743 seconds\n",
      "First action duration = 0.5398 seconds\n",
      "Second action duration = 0.0402 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Create an input\n",
    "begin = time.time()\n",
    "str_text = \" \".join(get_sentences(50000))\n",
    "lst_text = str_text.split(\" \")\n",
    "rdd_text = sc.parallelize(lst_text)\n",
    "print(\"Initialization duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Transformation, mapping and reducing\n",
    "begin = time.time()\n",
    "rdd_text_map = rdd_text.map(lambda w: (w, 1))\n",
    "rdd_text_red = rdd_text_map.reduceByKey(lambda a, b: a + b)\\\n",
    ".persist(StorageLevel.MEMORY_ONLY)\n",
    "print(\"Transformations duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Action to collect\n",
    "begin = time.time()\n",
    "rdd_text_red.collect()\n",
    "print(\"First action duration = {:.4f} seconds\".format(time.time() - begin))\n",
    "\n",
    "# Action to collect\n",
    "begin = time.time()\n",
    "rdd_text_red.collect()\n",
    "print(\"Second action duration = {:.4f} seconds\".format(time.time() - begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".unpersist() removes the files from cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast is a read only rdd. \"creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_var = sc.broadcast([1, 2, 3])\n",
    "broadcast_var.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast_var.map(lambda s: s + 1)\n",
    "# ---------------------------------------------------------------------------\n",
    "# AttributeError                            Traceback (most recent call last)\n",
    "# <ipython-input-65-7ac03e92578c> in <module>\n",
    "# ----> 1 broadcast_var.map(lambda s: s + 1)\n",
    "# \n",
    "# AttributeError: 'Broadcast' object has no attribute 'map'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serves as a global variable than can be increased. The tasks are in several different machines, a local variable can be incremented but its value will be different among workers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"An accumulator is created from an initial value v by calling SparkContext.accumulator(v). Tasks running on a cluster can then add to it using the add method or the += operator. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't work with a simple variable. Even if it worked, this could be true in local mode but not in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inc_var(x):\n",
    "    global var\n",
    "    var = var + x\n",
    "    return x\n",
    "\n",
    "var = 0\n",
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: inc_var(x))\n",
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example is works with the same function and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "var = 0\n",
    "pd.DataFrame([1, 2, 3, 4]).applymap(lambda x: inc_var(x))\n",
    "var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Partitions vs cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*8kQ2nvNThU19D6ejFYS4uQ.jpeg\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_from https://medium.com/@millet.david.julien/how-to-use-pyspark-to-get-opencv-images-descriptors-with-aws-emr-dd3875503a75_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/700/1*z6nry1dMtIiaCzlpxgxagA.jpeg\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:300px;\"/>\n",
    "_from https://medium.com/@millet.david.julien/how-to-use-pyspark-to-get-opencv-images-descriptors-with-aws-emr-dd3875503a75_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task, partition, executor optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Cores per instances__ \\\n",
    "For  each instance of n cores, only n-1 can be used for parallelisation, one is used to manage the others. \\\n",
    "4 instances of 4 cores leads to 12 effective computing cores used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tasks per core__ \\\n",
    "If one task or partition is used per core the total process duration is the time of the longest partition treatment. If other partition is faster, the other cores have to wait and the resources is useless during this time (fig.1) To improve the variance in duration, a good thing is to have many tasks per cores, executed sequentially (fig.2) A good number depends on each project but the order of magnitude of tasks per cores is 3 to 5. \\\n",
    "4 instances of 4 cores leads to 12 effective cores, so to 36 partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Executors per core__ \\\n",
    "When there is a huge amount of cores per instance there can be many executors per nodes. We can list 3 main methods.\n",
    "* __One executor per core__ : There are a lot of executors and sometimes not enough memory for each executor.\n",
    "* __One executor per node__ : We don’t optimize the parallel possibilities in case of many cores.\n",
    "* __Five cores per executor__: It is the average solution most often chosen.\n",
    "\n",
    "4 instances of 4 cores leads to 12 effective computing cores used, so 2 executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/687/1*7EcqpIX04bss2V3DgPcloQ.jpeg\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_from https://medium.com/@millet.david.julien/how-to-use-pyspark-to-get-opencv-images-descriptors-with-aws-emr-dd3875503a75_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/682/1*GbK9baZf0AJUKBKnyBkqPw.jpeg\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_from https://medium.com/@millet.david.julien/how-to-use-pyspark-to-get-opencv-images-descriptors-with-aws-emr-dd3875503a75_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The spark options to launch the code are__\n",
    "* __num-executors__ : Total number of executors.\n",
    "* __executor-cores__ : Number of cores per executors.\n",
    "* __executor-memory__ : memory per executor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make an example with an architecture of 5 instances of 11 cores, with 64 GB per instance. \\\n",
    "With this solution we take the option __--executor-cores=5__. The number of executors is calculated with the number of useful cores. One core per instance is used to cluster management. It remains here 5∗(11−1)=50 cores. \\\n",
    "With five cores per executors, the number of executors is 50÷5=10. So __--num-executors=10__. \\\n",
    "The memory per executor is 64∗5÷10=32. We remove from this generally 7%. We calculate it 32∗(1−0.07)=29.76. We can set __--executor-memory=29GB__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A point to consider to choose the number of cores is the memory size of partitions. The maximum partition size is 128MB. In the upper example of 36 partitions, the dataset shouldn't excess 4.5Go. \\\n",
    "Concerning memory uses, the shuffle blocks can not be larger than 2GB. This means the total process of reducing by executors can not exceed 2GB. This is a reason of why no more than 5 cores per executors is choosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second example. I have a 100Gb dataset. what could I do ? \\\n",
    "First is to get the number of partitions of 128MB, 782.  \\\n",
    "Get the number of cores, 3 partitions per cores, so 261 cores. \\\n",
    "Best to have 5 cores per executors so 52.2 executors. \\\n",
    "If is needed 1Gb per executor \\\n",
    "So a solution can be to use 26 instances of 11 cores. 10 are used in each instance. The memory per instances has to be 2Gb+7%. The parameters to send are __--executor-cores=5__, __--num-executors=52__, __--executor-memory=1GB__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codes taken from this documentation: \\\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a local StreamingContext if it has not been launched previously\n",
    "try:\n",
    "    conf = SparkConf().setAppName(\"NetworkWordCount\").setMaster(\"local[2]\")\n",
    "    sc = SparkContext(conf=SparkConf())\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "\n",
    "# Need to use SparkSession(sc) to create DataFrame\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader & Writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read one text file or the whole directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the line one',\n",
       " 'This is the second line',\n",
       " \"Like the spoon the third line doesn't exists\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read one file\n",
    "dist_file = sc.textFile(\"data/first_document.txt\")\n",
    "dist_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proceed an action on the RDD , count caracters\n",
    "line_lengths = dist_file.map(lambda s: len(s))\n",
    "line_lengths.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first line of the second document',\n",
       " 'This is the second line',\n",
       " 'This is the line one',\n",
       " 'This is the second line',\n",
       " \"Like the spoon the third line doesn't exists\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read many files, stack the lines\n",
    "dist_file = sc.textFile(\"./data/*.txt\")\n",
    "dist_file.map(lambda file: sc.textFile(file))\n",
    "dist_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_lengths = dist_file.map(lambda s: len(s))\n",
    "line_lengths.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and save a RDD as squence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rdd\n",
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rdd saved if exists. saveAsSequenceFile can't overwrite \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "\n",
    "onlyfolders = [f for f in listdir(\"./data/\") if not isfile(join(\"./data/\", f))]\n",
    "if \"rdd_saved\" in onlyfolders:\n",
    "    shutil.rmtree(\"./data/rdd_saved\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RDD\n",
    "rdd.saveAsSequenceFile(\"./data/rdd_saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 'aaa'), (2, 'aa'), (1, 'a')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read RDD\n",
    "sc.sequenceFile(\"data/rdd_saved\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and save a dataframe - csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rdd\n",
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv specific function\n",
    "df \\\n",
    ".repartition(1) \\\n",
    ".write \\\n",
    ".mode('overwrite') \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".csv(\"data/csv_saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic  save function\n",
    "df \\\n",
    ".repartition(1) \\\n",
    ".write \\\n",
    ".format('csv')\\\n",
    ".mode('overwrite')\\\n",
    ".option(\"header\", \"true\") \\\n",
    ".save(\"data/csv_saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the csv with partitions in a folder. If repartition is set to 1, there are only one partition so one csv in the resulting folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|_c0|_c1|\n",
      "+---+---+\n",
      "| _1| _2|\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read with the generic load function\n",
    "df = spark\\\n",
    ".read\\\n",
    ".format('csv')\\\n",
    ".load(\"data/csv_saved\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark \\\n",
    ".read \\\n",
    ".format('csv') \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".load(\"data/csv_saved\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read with the specific csv function\n",
    "df = spark \\\n",
    ".read \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".csv(\"data/csv_saved\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and save a json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_strings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "otherPeopleRDD = sc.parallelize(json_strings)\n",
    "otherPeopleRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = otherPeopleRDD.toDF()\n",
    "# df.show()\n",
    "# generates error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|[Columbus, Ohio]| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read a serialized json string\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "otherPeople.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and save a parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "df = rdd.toDF([\"a\", \"b\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"a\") \\\n",
    ".write \\\n",
    ".format('parquet')\\\n",
    ".mode('overwrite')\\\n",
    ".save(\"data/pq_saved.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  2|\n",
      "|  3|\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark \\\n",
    ".read \\\n",
    ".format('parquet')\\\n",
    ".load(\"data/pq_saved.parquet\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of parquet is to save place but also to be requested on writing and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n",
      "+---+\n",
      "|  b|\n",
      "+---+\n",
      "| aa|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "df = rdd.toDF([\"a\", \"b\"])\n",
    "df.show()\n",
    "\n",
    "df.select(\"a\", \"b\") \\\n",
    ".write \\\n",
    ".partitionBy(\"a\") \\\n",
    ".format('parquet')\\\n",
    ".mode('overwrite')\\\n",
    ".save(\"data/pq_saved.parquet\")\n",
    "\n",
    "df = spark.read.parquet(\"data/pq_saved.parquet/a=2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# an other solution is to load as df then sql on df\n",
    "df = spark.read.format('parquet').load(\"data/pq_saved.parquet\")\n",
    "df.createOrReplaceTempView(\"parquetFile\")\n",
    "a = spark.sql(\"SELECT a FROM parquetFile WHERE a=2\")\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame & Column Columns and expressions, Transformations, Actions, Rows,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "df = rdd.toDF([\"a\", \"b\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"a\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Equivalent\n",
    "from pyspark.sql.functions import col\n",
    "df.select([\"a\", \"b\"]).show()\n",
    "df.select(\"a\", \"b\").show()\n",
    "df.select(col(\"a\"), col(\"b\")).show()\n",
    "\n",
    "from pyspark.sql.functions import column\n",
    "df.select(column(\"a\"), column(\"b\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|length(b)|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|length(b)|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|length(b)|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select with expression\n",
    "from pyspark.sql.functions import length\n",
    "df.select(length(df.b)).show()\n",
    "df.select(length(col(\"b\"))).show()\n",
    "# or\n",
    "df.selectExpr(\"length(b)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|upper(b)|\n",
      "+--------+\n",
      "|       A|\n",
      "|      AA|\n",
      "|     AAA|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|upper(b)|\n",
      "+--------+\n",
      "|       A|\n",
      "|      AA|\n",
      "|     AAA|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|upper(b)|\n",
      "+--------+\n",
      "|       A|\n",
      "|      AA|\n",
      "|     AAA|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "df.select(upper(\"b\")).show()\n",
    "df.select(upper(col(\"b\"))).show()\n",
    "df.select(upper(column(\"b\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame filter rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"a\") > 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df.a > 2].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"a\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((col(\"a\").isNotNull()) & (col(\"a\") > 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|  Joe| 25|\n",
      "|Smith| 27|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter on condition removes na values\n",
    "df = spark.createDataFrame([['Joe',25], ['Smith',27], [None,28], ['Sharon',33]],\n",
    "                           ('name','age'))\n",
    "df = df.filter(col(\"age\") < 30).filter(col(\"age\") > 20)\n",
    "df = df.filter(col(\"name\").isNotNull())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop, rename, replace columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "df = rdd.toDF([\"a\", \"b\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n",
      "+---+\n",
      "|  b|\n",
      "+---+\n",
      "|  a|\n",
      "| aa|\n",
      "|aaa|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop column\n",
    "df.drop(\"c\").show() # Still works\n",
    "df.drop(\"a\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([['John','Doe','25','2800'],\n",
    "                            ['Joe','Smith','24.0','2600']],\n",
    "                           ('fname','lname','age','sales'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+----+\n",
      "|fname|lname| age|sales|age2|\n",
      "+-----+-----+----+-----+----+\n",
      "| John|  Doe|  25| 2800|12.5|\n",
      "|  Joe|Smith|24.0| 2600|12.0|\n",
      "+-----+-----+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a column, with eventually an expression\n",
    "df.withColumn(\"age2\", col(\"age\") * 0.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-----+\n",
      "|fname|lname|age|sales|\n",
      "+-----+-----+---+-----+\n",
      "| John|  Doe| 25| 2800|\n",
      "|  Joe|Smith| 24| 2600|\n",
      "+-----+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df.withColumn(\"age\", col(\"age\").cast(IntegerType())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+\n",
      "|fname|lname| Age|sales|\n",
      "+-----+-----+----+-----+\n",
      "| John|  Doe|  25| 2800|\n",
      "|  Joe|Smith|24.0| 2600|\n",
      "+-----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename a column. \n",
    "df.withColumnRenamed(\"age\", \"Age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace value, replace na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|  aa|\n",
      "|  3|null|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['1','a'], ['2', 'aa'], ['3', None]],\n",
    "                           ['a','b'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|  a|   b|\n",
      "+---+----+\n",
      "|  1|   a|\n",
      "|  2|  bb|\n",
      "|  3|null|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace value. Why .na ??\n",
    "df.na.replace(\"aa\", \"bb\", subset=[\"b\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3| ab|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace na\n",
    "df.na.fill(\"ab\", subset=[\"b\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3| ab|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same to replace na\n",
    "df.fillna(\"ab\", subset=[\"b\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+\n",
      "|fname|lname| age|sales|\n",
      "+-----+-----+----+-----+\n",
      "| John|  Doe|  25| 2800|\n",
      "|  Joe|Smith|24.0| 2600|\n",
      "+-----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['John','Doe','25','2800'],\n",
    "                            ['Joe','Smith','24.0','2600']],\n",
    "                           ['fname','lname','age','sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+\n",
      "|fname|lname| age|sales|\n",
      "+-----+-----+----+-----+\n",
      "| John|  Doe|  25| 2800|\n",
      "|  Joe|Smith|24.0| 2600|\n",
      "+-----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['John','Doe','25','2800'],\n",
    "                            ['Joe','Smith','24.0','2600']],\n",
    "                           ('fname','lname','age','sales')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+-----+\n",
      "|fname|lname| age|sales|\n",
      "+-----+-----+----+-----+\n",
      "| John|  Doe|  25| 2800|\n",
      "|  Joe|Smith|24.0| 2600|\n",
      "+-----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['John','Doe','25','2800'],\n",
    "                            ['Joe','Smith','24.0','2600']])\\\n",
    ".toDF('fname','lname','age','sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+-----+\n",
      "|test|lname| age|sales|\n",
      "+----+-----+----+-----+\n",
      "|John|  Doe|  25| 2800|\n",
      "| Joe|Smith|24.0| 2600|\n",
      "+----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['John','Doe','25','2800'],\n",
    "                            ['Joe','Smith','24.0','2600']],\n",
    "                           ('fname','lname','age','sales'))\\\n",
    ".withColumnRenamed('fname', 'test').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,IntegerType,true)))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA = StructType([\n",
    "            StructField(\"name\",StringType(),True),\n",
    "            StructField(\"dept\",StringType(),True),\n",
    "            StructField(\"salary\",IntegerType(),True)\n",
    "])\n",
    "\n",
    "dfEmp = spark.createDataFrame([\n",
    "                    ['Tony',\"HR\",8000],\n",
    "                    ['Mona',None,10000],\n",
    "                    ['Jill','Sls',5000],\n",
    "                    ['Tim','Admin',7000]],SCHEMA)\n",
    "\n",
    "SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,IntegerType,true)))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfEmp.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'dept', 'salary']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA.fieldNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,IntegerType,true),StructField(salary,IntegerType,true)))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA.add(StructField(\"salary\",IntegerType(),True))\n",
    "SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(dept,StringType,true),StructField(salary,IntegerType,true),StructField(salary,IntegerType,true),StructField(salary,IntegerType,true)))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA.add(\"salary\",IntegerType(),True)\n",
    "SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "cust = Row(\"fname\",\"lname\")\n",
    "custRow = cust(\"John\",\"Doe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('John', 'Doe')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custRow[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Groupby, Grouped data methods, Aggregate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge, joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  b|\n",
      "|  2| bb|\n",
      "|  3|bbb|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "|  a|  c|\n",
      "+---+---+\n",
      "|  1|  c|\n",
      "|  2| cc|\n",
      "|  3|ccc|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(range(1, 4)).map(lambda x: (x, \"b\" * x))\n",
    "df1 = rdd1.toDF([\"a\", \"b\"])\n",
    "df1.show()\n",
    "\n",
    "rdd2 = sc.parallelize(range(1, 4)).map(lambda x: (x, \"c\" * x))\n",
    "df2 = rdd2.toDF([\"a\", \"c\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  3|bbb|ccc|\n",
      "|  2| bb| cc|\n",
      "|  1|  b|  c|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "df_res = spark.sql(\"SELECT df1.a, b, c FROM df1 INNER JOIN df2 ON df1.a=df2.a\")\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  3|bbb|ccc|\n",
      "|  2| bb| cc|\n",
      "|  1|  b|  c|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_res = df1.join(df2, [\"a\"], \"inner\")\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  a|  b|  a|  c|\n",
      "+---+---+---+---+\n",
      "|  3|bbb|  3|ccc|\n",
      "|  2| bb|  2| cc|\n",
      "|  1|  b|  1|  c|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_res = df1.join(df2, df1.a == df2.a, \"inner\")\n",
    "df_res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  b|\n",
      "|  2| bb|\n",
      "|  3|bbb|\n",
      "|  1|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 2, 3, 1]).map(lambda x: (x, \"b\" * x))\n",
    "df1 = rdd1.toDF([\"a\", \"b\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  a|count|\n",
      "+---+-----+\n",
      "|  3|    1|\n",
      "|  2|    1|\n",
      "|  1|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy([\"a\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  a|count|\n",
      "+---+-----+\n",
      "|  3|    1|\n",
      "|  2|    1|\n",
      "|  1|    2|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1.groupBy(col(\"a\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|  a|  b|count|\n",
      "+---+---+-----+\n",
      "|  3|bbb|    1|\n",
      "|  1|  b|    2|\n",
      "|  2| bb|    1|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1.groupBy(col(\"a\"), col(\"b\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(col(\"a\"), col(\"b\")).count().select(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|double|\n",
      "+------+\n",
      "|     2|\n",
      "|     4|\n",
      "|     2|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy(col(\"a\"), col(\"b\")).count().selectExpr(\"(count * 2) as double\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetimes Dates, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+\n",
      "|col_name|     value|    format|\n",
      "+--------+----------+----------+\n",
      "|    a   |2018-01-01|yyyy-MM-dd|\n",
      "|       b|2018-02-02|yyyy-MM-dd|\n",
      "|       c|02-02-2018|dd-MM-yyyy|\n",
      "+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sc.parallelize([(' a   ','2018-01-01','yyyy-MM-dd'),\n",
    "                      ('b','2018-02-02','yyyy-MM-dd'),\n",
    "                      ('c','02-02-2018','dd-MM-yyyy')]).toDF(\n",
    "                    [\"col_name\",\"value\",\"format\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+\n",
      "|col_name|     value|    format|     test3|\n",
      "+--------+----------+----------+----------+\n",
      "|    a   |2018-01-01|yyyy-MM-dd|2018-01-01|\n",
      "|       b|2018-02-02|yyyy-MM-dd|2018-02-02|\n",
      "|       c|02-02-2018|dd-MM-yyyy|2018-02-02|\n",
      "+--------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.withColumn(\"test3\", expr(\"to_date(value, format)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+----------+\n",
      "|col_name|     value|    format|     test3|\n",
      "+--------+----------+----------+----------+\n",
      "|    a   |2018-01-01|yyyy-MM-dd|2018-01-01|\n",
      "|       b|2018-02-02|yyyy-MM-dd|2018-01-01|\n",
      "|       c|02-02-2018|dd-MM-yyyy|2018-01-01|\n",
      "+--------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"test3\", to_date(lit(\"20180101\"), \"yyyyMMdd\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-----+\n",
      "|col_name|     value|    format|test3|\n",
      "+--------+----------+----------+-----+\n",
      "|    a   |2018-01-01|yyyy-MM-dd| null|\n",
      "|       b|2018-02-02|yyyy-MM-dd| null|\n",
      "|       c|02-02-2018|dd-MM-yyyy| null|\n",
      "+--------+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"test3\", to_date(\"value\", \"format\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Types String functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------+\n",
      "|col_name|     value|    format|a trimed|\n",
      "+--------+----------+----------+--------+\n",
      "|    a   |2018-01-01|yyyy-MM-dd|       a|\n",
      "|       b|2018-02-02|yyyy-MM-dd|       b|\n",
      "|       c|02-02-2018|dd-MM-yyyy|       c|\n",
      "+--------+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trim\n",
    "df.withColumn(\"a trimed\", trim(\"col_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+-------------+\n",
      "|col_name|     value|    format|a left trimed|\n",
      "+--------+----------+----------+-------------+\n",
      "|    a   |2018-01-01|yyyy-MM-dd|         a   |\n",
      "|       b|2018-02-02|yyyy-MM-dd|            b|\n",
      "|       c|02-02-2018|dd-MM-yyyy|            c|\n",
      "+--------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ltrim\n",
    "df.withColumn(\"a left trimed\", ltrim(\"col_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------+--------------+\n",
      "|col_name|     value|    format|a right trimed|\n",
      "+--------+----------+----------+--------------+\n",
      "|    a   |2018-01-01|yyyy-MM-dd|             a|\n",
      "|       b|2018-02-02|yyyy-MM-dd|             b|\n",
      "|       c|02-02-2018|dd-MM-yyyy|             c|\n",
      "+--------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rtrim\n",
    "df.withColumn(\"a right trimed\", rtrim(\"col_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "                    ['William T',\"HR\",8000],\n",
    "                    ['Bob H',\"HR\",8000],\n",
    "                    ['Lisa M','HR',5000],\n",
    "                    ['Marie A','ADMIN',7000]],('name','dept','salary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------------+\n",
      "|     name| dept|salary|   splitName|\n",
      "+---------+-----+------+------------+\n",
      "|William T|   HR|  8000|[William, T]|\n",
      "|    Bob H|   HR|  8000|    [Bob, H]|\n",
      "|   Lisa M|   HR|  5000|   [Lisa, M]|\n",
      "|  Marie A|ADMIN|  7000|  [Marie, A]|\n",
      "+---------+-----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "df = df.withColumn(\"splitName\", split(col(\"name\"), \" \"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------------+-----------+\n",
      "|     name| dept|salary|   splitName|explodeName|\n",
      "+---------+-----+------+------------+-----------+\n",
      "|William T|   HR|  8000|[William, T]|    William|\n",
      "|William T|   HR|  8000|[William, T]|          T|\n",
      "|    Bob H|   HR|  8000|    [Bob, H]|        Bob|\n",
      "|    Bob H|   HR|  8000|    [Bob, H]|          H|\n",
      "|   Lisa M|   HR|  5000|   [Lisa, M]|       Lisa|\n",
      "|   Lisa M|   HR|  5000|   [Lisa, M]|          M|\n",
      "|  Marie A|ADMIN|  7000|  [Marie, A]|      Marie|\n",
      "|  Marie A|ADMIN|  7000|  [Marie, A]|          A|\n",
      "+---------+-----+------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df = df.withColumn(\"explodeName\", explode(col(\"splitName\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          empDetails|\n",
      "+--------------------+\n",
      "|[1 -> John, 2 -> ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, map_values\n",
    "df = df.select(create_map(lit(\"1\"), lit(\"John\"), lit(\"2\"),lit(\"Marie\")).\\\n",
    "               alias(\"empDetails\")).limit(1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         keys|\n",
      "+-------------+\n",
      "|[John, Marie]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(map_values(\"empDetails\").alias(\"keys\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   ids|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "df.select(map_keys(\"empDetails\").alias(\"ids\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Defined Functions User-defined functions, Vectorized UDFs, Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 8]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myFunc(s):\n",
    "        words = s.split(\" \")\n",
    "        return len(words)\n",
    "\n",
    "sc.textFile(\"data/first_document.txt\").map(myFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2| aa|\n",
      "|  3|aaa|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))\n",
    "df = rdd.toDF([\"a\", \"b\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  a|\n",
      "+---+\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sql_view\")\n",
    "a = spark.sql(\"SELECT a FROM sql_view WHERE a=2\")\n",
    "a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming \n",
    "Most of examples are taken from \\\n",
    "_https://spark.apache.org/docs/latest/streaming-programming-guide.html_ \\\n",
    "I tried them and completed them to be able to execute them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.\"\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-arch.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import StreamingContext, which is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and batch interval of 1 second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "from pyspark import SparkContext, SparkConf\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname (e.g. localhost) and port (e.g. 9999)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lines DStream represents the stream of data that will be received from the data server. Each record in this DStream is a line of text. Next, we want to split the lines by space into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap is a one-to-many DStream operation that creates a new DStream by generating multiple new records from each record in the source DStream. In this case, each line will be split into multiple words and the stream of words is represented as the words DStream. Next, we want to count these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words DStream is further mapped (one-to-one transformation) to a DStream of (word, 1) pairs, which is then reduced to get the frequency of words in each batch of data. Finally, wordCounts.pprint() will print a few of the counts generated every second.\n",
    "\n",
    "Note that when these lines are executed, Spark Streaming only sets up the computation it will perform when it is started, and no real processing has started yet. To start the processing after all the transformations have been setup, we finally call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:09\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()             # Start the computation\n",
    "#ssc.awaitTermination()  # Wait for the computation to terminate due to manual exit or error\n",
    "import time\n",
    "time.sleep(3)\n",
    "ssc.stop(stopSparkContext=False)  # Stop listening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On an other terminal run the command to listen the port\n",
    "\n",
    "nc -l 10222"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"After a context is defined, you have to do the following.\n",
    "\n",
    "    Define the input sources by creating input DStreams.\n",
    "    Define the streaming computations by applying transformation and output operations to DStreams.\n",
    "    Start receiving data and processing it using streamingContext.start().\n",
    "    Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().\n",
    "    The processing can be manually stopped using streamingContext.stop().\n",
    "\n",
    "Points to remember:\n",
    "\n",
    "    Once a context has been started, no new streaming computations can be set up or added to it.\n",
    "    Once a context has been stopped, it cannot be restarted.\n",
    "    Only one StreamingContext can be active in a JVM at the same time.\n",
    "    stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.\n",
    "    A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.\"\n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretized stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stream is defined as a serie of discrete RDDs inscreasing by time and where all the transformation are applied \\\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "_image from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input / output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input DStreams and Receivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\" Input DStreams are DStreams representing the stream of input data received from streaming sources. In the quick example, lines was an input DStream as it represented the stream of data received from the netcat server. Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (Scala doc, Java doc) object which receives the data from a source and stores it in Spark’s memory for processing.\n",
    "\n",
    "Spark Streaming provides two categories of built-in streaming sources.\n",
    "\n",
    "    Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.\n",
    "    Advanced sources: Sources like Kafka, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.\n",
    "\n",
    "We are going to discuss some of the sources present in each category later in this section.\n",
    "\n",
    "Note that, if you want to receive multiple streams of data in parallel in your streaming application, you can create multiple input DStreams (discussed further in the Performance Tuning section). This will create multiple receivers which will simultaneously receive multiple data streams. But note that a Spark worker/executor is a long-running task, hence it occupies one of the cores allocated to the Spark Streaming application. Therefore, it is important to remember that a Spark Streaming application needs to be allocated enough cores (or threads, if running locally) to process the received data, as well as to run the receiver(s).\n",
    "\n",
    "Points to remember\n",
    "\n",
    "    When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using an input DStream based on a receiver (e.g. sockets, Kafka, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. Hence, when running locally, always use “local[n]” as the master URL, where n > number of receivers to run (see Spark Properties for information on how to set the master).\n",
    "\n",
    "    Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. Otherwise the system will receive data, but not be able to process it. \"\n",
    "\n",
    "_by https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get files like text log, when they arrive in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Utilisateurs\\\\Millet\\\\AppData\\\\Roaming\\\\IBM Watson Studio\\\\logs\\\\\"\n",
    "#ssc = StreamingContext.textFileStream(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files must be in the same data format.\n",
    "\n",
    "A file is considered part of a time period based on its modification time, not its creation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queue of RDD for test purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "queueOfRDDs = [sc.range(0, 1000),\n",
    "               sc.range(1000, 3000),\n",
    "               sc.range(2000, 4000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_received = ssc.queueStream(queueOfRDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_received.count().pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:10\n",
      "-------------------------------------------\n",
      "1000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:11\n",
      "-------------------------------------------\n",
      "2000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:12\n",
      "-------------------------------------------\n",
      "2000\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:13\n",
      "-------------------------------------------\n",
      "0\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:14\n",
      "-------------------------------------------\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "#ssc.awaitTermination()\n",
    "\n",
    "import time\n",
    "\n",
    "# Note Changes to the queue after the stream is created will not be recognized. \n",
    "# https://spark.apache.org/docs/latest/api/python/pyspark.streaming.html?highlight=queue\n",
    "queueOfRDDs.append(sc.range(0, 100))\n",
    "\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 kinds of receivers\n",
    "\n",
    "Reliable Receiver - A reliable receiver correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication. (kafka...)\n",
    "\n",
    "Unreliable Receiver - An unreliable receiver does not send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when one does not want or need to go into the complexity of acknowledgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on DStreams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Transformation\tMeaning\n",
    "* __map(func)__ \t    Return a new DStream by passing each element of the source DStream through a function func.\n",
    "* __flatMap(func)__ \tSimilar to map, but each input item can be mapped to 0 or more output items.\n",
    "* __filter(func)__ \tReturn a new DStream by selecting only the records of the source DStream on which func returns true.\n",
    "* __repartition(numPartitions)__ \tChanges the level of parallelism in this DStream by creating more or fewer partitions.\n",
    "* __union(otherStream)__ \tReturn a new DStream that contains the union of the elements in the source DStream and otherDStream.\n",
    "* __count()__ \tReturn a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.\n",
    "* __reduce(func)__ \tReturn a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.\n",
    "* __countByValue()__ \tWhen called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.\n",
    "* __reduceByKey(func, [numTasks])__ \tWhen called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "* __join(otherStream, [numTasks])__ \tWhen called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.\n",
    "* __cogroup(otherStream, [numTasks])__ \tWhen called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.\n",
    "* __transform(func)__ \tReturn a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary * RDD operations on the DStream.\n",
    "* __updateStateByKey(func)__ \tReturn a new \"state\" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.\" \\\n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical transformations example\n",
    "Dummy example: Get the number of letters for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:15\n",
      "-------------------------------------------\n",
      "21\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:16\n",
      "-------------------------------------------\n",
      "27\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:17\n",
      "-------------------------------------------\n",
      "44\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:19\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of map/reduce/count on StreamingContext\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "\n",
    "                     \n",
    "# Map the words with their lengths - cache because lineLengths is called with 2 actions\n",
    "words = rdd_received.flatMap(lambda line: line.split(\" \"))\n",
    "# format the RDD with key value pairs\n",
    "line_lengths = words.map(lambda word : len(word))\n",
    "# Count the words\n",
    "total_length = line_lengths.reduce(lambda a, b: a + b)\n",
    "# Display result\n",
    "total_length.pprint()\n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updateStateByKey transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: get the number of occurence of each word in a stream. not per document. \\\n",
    "Only one result is sent, and is updated for each new rdd coming. \\\n",
    "After the last RDD, the result is provided each second. \\\n",
    "Checkpoint use is necessaty here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:20\n",
      "-------------------------------------------\n",
      "('first', 1)\n",
      "('is', 1)\n",
      "('This', 1)\n",
      "('my', 1)\n",
      "('document', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:21\n",
      "-------------------------------------------\n",
      "('first', 1)\n",
      "('a', 1)\n",
      "('is', 2)\n",
      "('longer', 1)\n",
      "('This', 2)\n",
      "('my', 1)\n",
      "('document', 2)\n",
      "('second', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:22\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('first', 1)\n",
      "('a', 3)\n",
      "('which', 1)\n",
      "('is', 4)\n",
      "('longer', 1)\n",
      "('know', 1)\n",
      "('This', 3)\n",
      "('my', 1)\n",
      "('document', 4)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:23\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('first', 1)\n",
      "('a', 3)\n",
      "('which', 1)\n",
      "('is', 4)\n",
      "('longer', 1)\n",
      "('know', 1)\n",
      "('This', 3)\n",
      "('my', 1)\n",
      "('document', 4)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:24\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('first', 1)\n",
      "('a', 3)\n",
      "('which', 1)\n",
      "('is', 4)\n",
      "('longer', 1)\n",
      "('know', 1)\n",
      "('This', 3)\n",
      "('my', 1)\n",
      "('document', 4)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of updateStateByKey use\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "def update_word_count(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "        runningCount = 0\n",
    "    return sum(newValues, runningCount) \n",
    "\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"data/checkpoint_directory\")\n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "                    \n",
    "# Map the words with their lengths - cache because lineLengths is called with 2 actions\n",
    "words = rdd_received.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# format the RDD with key value pairs\n",
    "line_lengths = words.map(lambda word : (word, 1))\n",
    "\n",
    "# Count the words\n",
    "word_count = line_lengths.reduceByKey(lambda a, b : a + b)\n",
    "# word_count.pprint() # If the occurence of words per document is needed\n",
    "\n",
    "# update each rdd state  by key when other rdd comes\n",
    "total_word_count = word_count.updateStateByKey(update_word_count)\n",
    "\n",
    "# Display result\n",
    "total_word_count.pprint()\n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform(func) operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The transform operation (along with its variations like transformWith) allows arbitrary RDD-to-RDD functions to be applied on a DStream. It can be used to apply any RDD operation that is not exposed in the DStream API. For example, the functionality of joining every batch in a data stream with another dataset is not directly exposed in the DStream API. However, you can easily use transform to do this. This enables very powerful possibilities. For example, one can do real-time data cleaning by joining the input data stream with precomputed spam information (maybe generated with Spark as well) and then filtering based on it.\" \n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Get the number of word occurence in the whole stream, \\\n",
    "with a transformation to filter unwanted words \\\n",
    "The rdd with unwanted words will be joined and filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', (None, 1)),\n",
       " ('This', (1, 1)),\n",
       " ('document', (1, None)),\n",
       " ('document', (1, None)),\n",
       " ('my', (1, 1)),\n",
       " ('first', (1, None)),\n",
       " ('is', (1, 1))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('document', (1, None)), ('document', (1, None)), ('first', (1, None))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('document', 1), ('document', 1), ('first', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('document', 2), ('first', 1)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With a normal context outside a stream, the code would have been\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "\n",
    "# Initilize document and filter\n",
    "rdd_document = sc.parallelize([\"This document is my first document\"])\n",
    "rdd_unwanted_words = sc.parallelize([\"This\", \"is\", \"a\", \"my\"])\n",
    "\n",
    "\n",
    "# Map the words \n",
    "words = rdd_document.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# format the RDD with key value pairs\n",
    "lines = words.map(lambda word : (word, 1))\n",
    "unwanted = rdd_unwanted_words.map(lambda word : (word, 1))\n",
    "\n",
    "#  Outer Join on the 2 rdds \n",
    "doc_clnd = lines.fullOuterJoin(unwanted)\n",
    "display(doc_clnd.collect())\n",
    "doc_clnd2 = doc_clnd.filter(lambda x: x[1][0] == 1 and  x[1][1] is None)\n",
    "display(doc_clnd2.collect())\n",
    "doc_clnd3 = doc_clnd2.map(lambda x: (x[0], 1))\n",
    "display(doc_clnd3.collect())\n",
    "\n",
    "# reduce to get the count of word occurence\n",
    "word_count = doc_clnd3.reduceByKey(lambda a, b : a + b)\n",
    "word_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:26\n",
      "-------------------------------------------\n",
      "('document', 1)\n",
      "('document', 1)\n",
      "('first', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:26\n",
      "-------------------------------------------\n",
      "('first', 1)\n",
      "('document', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:27\n",
      "-------------------------------------------\n",
      "('longer', 1)\n",
      "('document', 1)\n",
      "('second', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:27\n",
      "-------------------------------------------\n",
      "('longer', 1)\n",
      "('document', 1)\n",
      "('second', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:28\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('which', 1)\n",
      "('document', 1)\n",
      "('document', 1)\n",
      "('know', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:28\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('which', 1)\n",
      "('know', 1)\n",
      "('document', 2)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:30\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With transfert in a stream\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "\n",
    "# RDD to filter\n",
    "rdd_unwanted_words = sc.parallelize([\"This\", \"is\", \"a\", \"my\", \"it\"])\n",
    "\n",
    "# Map the words \n",
    "words = rdd_received.flatMap(lambda line: line.split(\" \"))\n",
    "unwanted_words = rdd_unwanted_words.map(lambda word : (word, 1))\n",
    "\n",
    "# format the RDD with key value pairs\n",
    "line_lengths = words.map(lambda word : (word, 1))\n",
    "\n",
    "#line_lengths.pprint()\n",
    "cleanedDStream = line_lengths.transform(lambda rdd: rdd.fullOuterJoin(unwanted_words) \\\n",
    "                                                       .filter(lambda x: x[1][0] == 1 and  x[1][1] is None) \\\n",
    "                                                       .map(lambda x: (x[0], 1)))\n",
    "cleanedDStream.pprint()\n",
    "\n",
    "# Count the words\n",
    "word_count = cleanedDStream.reduceByKey(lambda a, b : a + b)\n",
    "word_count.pprint() \n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Spark Streaming also provides windowed computations, which allow you to apply transformations over a sliding window of data. The following figure illustrates this sliding window.\"\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-window.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"height:200px;\"/>\n",
    "     \n",
    "\"Some of the common window operations are as follows. All of these operations take the said two parameters - windowLength and slideInterval.\n",
    "* __window(windowLength, slideInterval)__ \tReturn a new DStream which is computed based on windowed batches of the source DStream.\n",
    "* __countByWindow(windowLength, slideInterval__) \tReturn a sliding window count of elements in the stream.\n",
    "* __reduceByWindow(func, windowLength, slideInterval)__ \tReturn a new single-element stream, created by aggregating elements in the stream over a sliding interval using func. The function should be associative and commutative so that it can be computed correctly in parallel.\n",
    "* __reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])__ \tWhen called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function func over batches in a sliding window. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "* __reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])__ A more efficient version of the above reduceByKeyAndWindow() where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter invFunc). Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument. Note that checkpointing must be enabled for using this operation.\n",
    "* __countByValueAndWindow(windowLength, slideInterval, [numTasks])__ \tWhen called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument. \n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Count the number of occurence of each words in the document \\\n",
    "This time does it reducing the last 2 documents in one rdd \\\n",
    "Only the first and the last (n+1) return a result from an unique rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:31\n",
      "-------------------------------------------\n",
      "('first', 1)\n",
      "('is', 1)\n",
      "('This', 1)\n",
      "('document', 2)\n",
      "('my', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:32\n",
      "-------------------------------------------\n",
      "('first', 1)\n",
      "('a', 1)\n",
      "('is', 2)\n",
      "('longer', 1)\n",
      "('This', 2)\n",
      "('document', 3)\n",
      "('my', 1)\n",
      "('second', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:33\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('a', 3)\n",
      "('which', 1)\n",
      "('is', 3)\n",
      "('longer', 1)\n",
      "('know', 1)\n",
      "('This', 2)\n",
      "('second', 1)\n",
      "('document', 3)\n",
      "('it', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:34\n",
      "-------------------------------------------\n",
      "(\"doesn't\", 1)\n",
      "('a', 2)\n",
      "('which', 1)\n",
      "('is', 2)\n",
      "('know', 1)\n",
      "('This', 1)\n",
      "('document', 2)\n",
      "('it', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:35\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example with reduceByKeyAndWindow in a stream\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "\n",
    "# Map the words \n",
    "words = rdd_received.flatMap(lambda line: line.split(\" \"))\n",
    "# format the RDD with key value pairs\n",
    "line_lengths = words.map(lambda word : (word, 1))\n",
    "# Count the words by window, reduce last 2 second every second\n",
    "word_count = line_lengths.reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 2, 1)\n",
    "word_count.pprint() \n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Stream operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  join all the rdds of the stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stream1 = ... \\\n",
    "stream2 = ... \\\n",
    "joinedStream = stream1.join(stream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  join the rdds of specific windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "windowed_stream1 = stream1.window(20) \\\n",
    "windowed_stream2 = stream2.window(60) \\\n",
    "joined_stream = windowed_stream1.join(windowed_stream2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform: stream and rdd join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = ... # some RDD \\\n",
    "windowed_stream = stream.window(20) \\\n",
    "joinedStream = windowed_stream.transform(lambda rdd: rdd.join(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Operations on DStreams\n",
    "\n",
    "* __print()__ \tPrints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging.\n",
    "Python API This is called pprint() in the Python API.\n",
    "* __saveAsTextFiles(prefix, [suffix])__ \tSave this DStream's contents as text files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\".\n",
    "* __saveAsObjectFiles(prefix, [suffix])__ \tSave this DStream's contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\".\n",
    "Python API This is not available in the Python API.\n",
    "* __saveAsHadoopFiles(prefix, [suffix])__ \tSave this DStream's contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: \"prefix-TIME_IN_MS[.suffix]\".\n",
    "Python API This is not available in the Python API.\n",
    "* __foreachRDD(func)__ \tThe most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Patterns for using foreachRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a function to apply to each RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "def sendPartition(iter):\n",
    "    connection = createNewConnection()\n",
    "    for record in iter:\n",
    "        connection.send(record)\n",
    "    connection.close()\n",
    "\n",
    "# Initialize spark context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"])]\n",
    "\n",
    "# Initilize Streamcontext and execute te function for each rdd\n",
    "ssc = StreamingContext(sc, 1)\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "rdd_received.foreachRDD(lambda rdd: rdd.foreachPartition(sendPartition))\n",
    "\n",
    "# Launch, wait and stop\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame and SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "========= 2020-11-18 16:41:41 =========\n",
      "========= 2020-11-18 16:41:42 =========\n",
      "========= 2020-11-18 16:41:43 =========\n",
      "========= 2020-11-18 16:41:44 =========\n",
      "========= 2020-11-18 16:41:45 =========\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    # here you can do things about RDD or SQL...\n",
    "    rdd_count = rdd.map(lambda x: len(x))\n",
    "    rdd_count.show()\n",
    "    \n",
    "\n",
    "# Initialize spark context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"])]\n",
    "\n",
    "# Initilize Streamcontext and execute te function for each rdd\n",
    "ssc = StreamingContext(sc, 1)\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "rdd_received.foreachRDD(process)\n",
    "\n",
    "# Launch, wait and stop\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't do anything because the queueStream is manual and the function awaits new data coming..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching / Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to RDDs, DStreams also allow developers to persist the stream’s data in memory. That is, using the persist() method on a DStream will automatically persist every RDD of that DStream in memory. This is useful if the data in the DStream will be computed multiple times (e.g., multiple operations on the same data). For window-based operations like reduceByWindow and reduceByKeyAndWindow and state-based operations like updateStateByKey, this is implicitly true. Hence, DStreams generated by window-based operations are automatically persisted in memory, without the developer calling persist().\n",
    "\n",
    "For input streams that receive data over the network (such as, Kafka, sockets, etc.), the default persistence level is set to replicate the data to two nodes for fault-tolerance.\n",
    "\n",
    "Note that, unlike RDDs, the default persistence level of DStreams keeps the data serialized in memory. This is further discussed in the Performance Tuning section. More information on different persistence levels can be found in the Spark Programming Guide.\n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "\"A streaming application must operate 24/7 and hence must be resilient to failures unrelated to the application logic (e.g., system failures, JVM crashes, etc.). For this to be possible, Spark Streaming needs to checkpoint enough information to a fault- tolerant storage system such that it can recover from failures. There are two types of data that are checkpointed.\n",
    "\n",
    "* Metadata checkpointing - Saving of the information defining the streaming computation to fault-tolerant storage like HDFS. This is used to recover from failure of the node running the driver of the streaming application (discussed in detail later). Metadata includes:\n",
    " * Configuration - The configuration that was used to create the streaming application.\n",
    " * DStream operations - The set of DStream operations that define the streaming application.\n",
    " * Incomplete batches - Batches whose jobs are queued but have not completed yet.\n",
    "\n",
    "* Data checkpointing - Saving of the generated RDDs to reliable storage. This is necessary in some stateful transformations that combine data across multiple batches. In such transformations, the generated RDDs depend on RDDs of previous batches, which causes the length of the dependency chain to keep increasing with time. To avoid such unbounded increases in recovery time (proportional to dependency chain), intermediate RDDs of stateful transformations are periodically checkpointed to reliable storage (e.g. HDFS) to cut off the dependency chains.\n",
    "\n",
    "To summarize, metadata checkpointing is primarily needed for recovery from driver failures, whereas data or RDD checkpointing is necessary even for basic functioning if stateful transformations are used.\"\n",
    "\n",
    "_from https://spark.apache.org/docs/latest/streaming-programming-guide.html#window-operations_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context already setup\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:46\n",
      "-------------------------------------------\n",
      "This document is my first document\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:47\n",
      "-------------------------------------------\n",
      "This is a second longer document\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:48\n",
      "-------------------------------------------\n",
      "This is a document which doesn't know it is a document\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2020-11-18 16:41:51\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of use of checkpoints\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import time\n",
    "\n",
    "# launch spark and streaming context\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "except:\n",
    "    print(\"Context already setup\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# set checkpoint directory\n",
    "ssc.checkpoint(\"data/checkpoint_Directory2\")  \n",
    "\n",
    "# RDD reading different text documents\n",
    "queueOfRDDs = [sc.parallelize([\"This document is my first document\"]),\n",
    "               sc.parallelize([\"This is a second longer document\"]),\n",
    "               sc.parallelize([\"This is a document which doesn't know it is a document\"])]\n",
    "rdd_received = ssc.queueStream(queueOfRDDs)\n",
    "\n",
    "# set checkpoint Interval, 3 second\n",
    "rdd_received.checkpoint(3)\n",
    "\n",
    "# Execute an action\n",
    "rdd_received.pprint()\n",
    "\n",
    "# launch wait and terminate\n",
    "ssc.start()\n",
    "time.sleep(5)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for the part 5\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create spark context\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# delete files from previous runs\n",
    "!rm -f data/hmp.parquet*\n",
    "\n",
    "# download the file containing the data in PARQUET format\n",
    "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
    "!mv hmp.parquet data/hmp.parquet\n",
    "    \n",
    "# create a dataframe out of it\n",
    "df = spark.read.parquet('data/hmp.parquet')\n",
    "\n",
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML fit, transform, pipelines, estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+\n",
      "|  x|  y|  z|              source|      class|\n",
      "+---+---+---+--------------------+-----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
      "+---+---+---+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use fit, transform on StringIndexer estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+\n",
      "|  x|  y|  z|              source|      class|classIndex|\n",
      "+---+---+---+--------------------+-----------+----------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|\n",
      "+---+---+---+--------------------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer#, VectorAssembler, Normalizer, OneHotEncoderEstimator\n",
    "est_indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "model = est_indexer.fit(df)\n",
    "df_indexed = model.transform(df)\n",
    "df_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a second fit transform on OneHotEncoder estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "est_indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "est_encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "df_indexed = est_indexer.fit(df).transform(df)\n",
    "df_encoded = est_encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_encoded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pipeline to make different transformation in a row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "|  x|  y|  z|              source|      class|classIndex|   categoryVec|        features|       features_norm|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 49| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.20754716981132...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 22| 52| 35|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.20183486238532...|\n",
      "| 21| 52| 34|Accelerometer-201...|Brush_teeth|       6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.19626168224299...|\n",
      "+---+---+---+--------------------+-----------+----------+--------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define transformations\n",
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
    "encoder = OneHotEncoder(inputCol=\"classIndex\", outputCol=\"categoryVec\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"], outputCol=\"features\")\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n",
    "# Define pipeline\n",
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer])\n",
    "model = pipeline.fit(df)\n",
    "df_result = model.transform(df)\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|target|            features|\n",
      "+------+--------------------+\n",
      "|   6.0|[0.20754716981132...|\n",
      "|   6.0|[0.20754716981132...|\n",
      "|   6.0|[0.20183486238532...|\n",
      "|   6.0|[0.20183486238532...|\n",
      "|   6.0|[0.19626168224299...|\n",
      "+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean dataframe for the next part\n",
    "df_cleaned = df_result.drop(\"x\").drop(\"y\").drop(\"z\").drop(\"source\").drop(\"class\").drop(\"categoryVec\").drop(\"features\")\n",
    "df_cleaned = df_cleaned.withColumnRenamed(\"classIndex\", \"target\")\n",
    "df_cleaned = df_cleaned.withColumnRenamed(\"features_norm\", \"features\")\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection, evaluator, parameter grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and validation datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test data.\n",
    "df_learn, df_validation = df_cleaned.randomSplit([0.8, 0.2], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and estimate model with fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "estim_lr = LinearRegression(featuresCol = 'features', labelCol='target',\n",
    "                            maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model_lr = estim_lr.fit(df_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [9.276474699807274,0.0,-2.7557906328884165]\n",
      "Intercept: 3.2530328439765346\n",
      "RMSE: 3.509002\n",
      "r2: 0.086105\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(model_lr.coefficients))\n",
    "print(\"Intercept: %s\" % str(model_lr.intercept))\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = model_lr.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------------+\n",
      "|target|            features|        prediction|\n",
      "+------+--------------------+------------------+\n",
      "|   0.0|[0.0,0.4852941176...|1.8346111946957322|\n",
      "|   0.0|[0.0,0.4933333333...|1.8567655899797368|\n",
      "|   0.0|[0.0,0.4941176470...|1.8589269943976887|\n",
      "|   0.0|       [0.0,0.5,0.5]|1.8751375275323263|\n",
      "|   0.0|       [0.0,0.5,0.5]|1.8751375275323263|\n",
      "+------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "R Squared (R2) on test data = 0.0862592\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "df_prediction = model_lr.transform(df_validation)\n",
    "df_prediction.show(5)\n",
    "\n",
    "# Evaluate model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval_lr = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                              labelCol=\"target\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % eval_lr.evaluate(df_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.51257\n"
     ]
    }
   ],
   "source": [
    "# The prediction and evaluation can be done in one step\n",
    "test_result = model_lr.evaluate(df_validation)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.0862592\n"
     ]
    }
   ],
   "source": [
    "# To summarize:\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Train model\n",
    "estim_lr = LinearRegression(featuresCol = 'features', labelCol='target',\n",
    "                            maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model_lr = estim_lr.fit(df_learn)\n",
    "# Evaluate model\n",
    "df_prediction = model_lr.transform(df_validation)\n",
    "eval_lr = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                              labelCol=\"target\", metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % eval_lr.evaluate(df_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation selection to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,0.232,-0.017...|    0|\n",
      "|[0.0,0.111,-0.003...|    0|\n",
      "|[0.0,-0.391,-0.00...|    0|\n",
      "|[0.0,0.098,-0.012...|    0|\n",
      "|[0.0,0.026,-0.004...|    0|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "temp_df = spark.createDataFrame([Row(V4366=0.0, V4460=0.232, V4916=-0.017, V1495=-0.104, V1639=0.005, V1967=-0.008, V3049=0.177, V3746=-0.675, V3869=-3.451, V524=0.004, V5409=0), Row(V4366=0.0, V4460=0.111, V4916=-0.003, V1495=-0.137, V1639=0.001, V1967=-0.01, V3049=0.01, V3746=-0.867, V3869=-2.759, V524=0.0, V5409=0), Row(V4366=0.0, V4460=-0.391, V4916=-0.003, V1495=-0.155, V1639=-0.006, V1967=-0.019, V3049=-0.706, V3746=0.166, V3869=0.189, V524=0.001, V5409=0), Row(V4366=0.0, V4460=0.098, V4916=-0.012, V1495=-0.108, V1639=0.005, V1967=-0.002, V3049=0.033, V3746=-0.787, V3869=-0.926, V524=0.002, V5409=0), Row(V4366=0.0, V4460=0.026, V4916=-0.004, V1495=-0.139, V1639=0.003, V1967=-0.006, V3049=-0.045, V3746=-0.208, V3869=-0.782, V524=0.001, V5409=0)])\n",
    "trainingData=temp_df.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])\n",
    "trainingData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Initialize estimators, pipeline and evaluators\n",
    "estim_lr = LinearRegression(featuresCol = 'features', labelCol='target', \n",
    "                            maxIter=10) #, regParam=0.3, elasticNetParam=0.8)\n",
    "pipeline = Pipeline(stages=[estim_lr])\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                                labelCol=\"target\", metricName=\"r2\")\n",
    "\n",
    "# Define the different parameters to test\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(estim_lr.regParam, [0.3, 0.5]) \\\n",
    "    .addGrid(estim_lr.elasticNetParam, [0.5, 0.8]) \\\n",
    "    .build()\n",
    "\n",
    "# Run cross validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "cvModel = crossval.fit(df_learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6854132664170263\n",
      "[9.779454982679272,0.0,-4.111892840696731]\n",
      "0.3\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# print regression result and the best parameters found\n",
    "print(cvModel.bestModel.stages[0].intercept)\n",
    "print(cvModel.bestModel.stages[0].coefficients)\n",
    "print(cvModel.bestModel.stages[0].getRegParam())\n",
    "print(cvModel.bestModel.stages[0].getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.0898366\n"
     ]
    }
   ],
   "source": [
    "# CV model on the k folds, then train on the whole training set. Can be directly used for validation set\n",
    "df_prediction = cvModel.transform(df_validation)\n",
    "eval_lr = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                              labelCol=\"target\", \n",
    "                              metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % eval_lr.evaluate(df_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "797px",
    "left": "599px",
    "top": "180px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
